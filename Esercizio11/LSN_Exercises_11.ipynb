{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> Python Exercise 11 </span>  Igor Vasiljevic 11191A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:orange\"> Keras - Neural Network regression </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that, i performe the code on local python edit. So i will add the code .py inside the folder. The problem is that the kernel died on jupyter lab (even your code), so to avoid the problem (after a lot of time) i decide to add the program in .py (i test the code) and copy the code inside here and hopes that will do. And yes, i tried even the conda activate tf like in Ariel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview \n",
    "\n",
    "In this notebook our task will be to perform machine learning regression on noisy data with a Neural Network (NN).\n",
    "\n",
    "We will explore how the ability to fit depends on the structure of the NN. The goal is also to build intuition about why prediction is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction Problem\n",
    "\n",
    "Consider a probabilistic process that gives rise to labeled data $(x,y)$. The data is generated by drawing samples from the equation\n",
    "\n",
    "$$\n",
    "    y_i= f(x_i) + \\eta_i,\n",
    "$$\n",
    "\n",
    "where $f(x_i)$ is some fixed, but (possibly unknown) function, and $\\eta_i$ is a Gaussian, uncorrelate noise variable such that\n",
    "\n",
    "$$\n",
    "\\langle \\eta_i \\rangle=0 \\\\\n",
    "\\langle \\eta_i \\eta_j \\rangle = \\delta_{ij} \\sigma\n",
    "$$\n",
    "\n",
    "We will refer to the $f(x_i)$ as the **true features** used to generate the data. \n",
    "\n",
    "To make predictions, we will consider a NN that depends on its parameters, weights and biases. The functions that the NN can model respresent the **model class** that we are using to try to model the data and make predictions.\n",
    "\n",
    "To learn the parameters of the NN, we will train our models on a **training data set** and then test the effectiveness of the NN on a *different* dataset, the **validation data set**. The reason we must divide our data into a training and test dataset is that the point of machine learning is to make accurate predictions about new data we have not seen.\n",
    "\n",
    "To measure our ability to predict, we will learn our parameters by fitting our training dataset and then making predictions on our test data set. One common measure of predictive  performance of our algorithm is to compare the predictions,$\\{y_j^\\mathrm{pred}\\}$, to the true values $\\{y_j\\}$. A commonly employed measure for this is the sum of the mean square-error (MSE) on the test set:\n",
    "$$\n",
    "MSE= \\frac{1}{N_\\mathrm{test}}\\sum_{j=1}^{N_\\mathrm{test}} (y_j^\\mathrm{pred}-y_j)^2\n",
    "$$\n",
    "\n",
    "We will try to get a qualitative picture by examining plots on validation and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear fit\n",
    "\n",
    "We start by considering the very simple case:\n",
    "$$\n",
    "f(x)=2x+1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.1\n",
    "\n",
    "In order to make practice with NN, explore how does the previous linear regression depend on the number of epochs, $N_{\\mathrm{epochs}}$, the number of data points $N_{\\mathrm{train}}$ and on the noise $\\sigma$. Try to improve the previous result operating on these parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I tried $\\sigma$=[0.1,0.3,0.5,0.7]; epochs=[50;100,150,200], Ntrain=[100,500,1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the results for $\\sigma$=0.3, epochs=200, Ntrain=500;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 11 the file it's named Esercizio1.1.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#Function to generate data for the function\n",
    "def generate_data(num_samples, noise_level):\n",
    "    X = np.linspace(0, 1, num_samples)\n",
    "    y = 5*X + 2 + np.random.normal(0, noise_level, num_samples)\n",
    "    return X, y\n",
    "\n",
    "#Function to build the model with fixed configurations\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=1, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "#Function to train the model\n",
    "def train_model(model, X_train, y_train, num_epochs):\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, verbose=0)\n",
    "    return history\n",
    "\n",
    "#Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "#Parameters\n",
    "num_epochs = 200\n",
    "num_train_samples = 500\n",
    "noise_level = 0.3\n",
    "#Generate noisy data\n",
    "X, y = generate_data(num_train_samples, noise_level)\n",
    "#Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#Build the model\n",
    "model = build_model()\n",
    "#Train the model\n",
    "train_model(model, X_train, y_train, num_epochs)\n",
    "\n",
    "#Evaluate the model\n",
    "mse = evaluate_model(model, X_test, y_test)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results\n",
    "plt.scatter(X_test, y_test, color='blue', label='True Data')\n",
    "plt.scatter(X_test, model.predict(X_test), color='red', label='Predicted Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regression using Neural Network')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this program, the generate_data function generates noisy data with a specified number of samples and noise level. The build_model function creates a simple neural network model with two dense layers. The train_model function trains the model on the training data for a given number of epochs. The evaluate_model function calculates the mean squared error (MSE) between the predicted and true values on the test data.\n",
    "\n",
    "## After training the model, the program evaluates its performance by calculating the MSE on the test data and plots the true data points as well as the predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 11 the file it's named Esercizio1.2.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.2\n",
    "\n",
    "Try to extend the model to obtain a reasonable fit of the following polynomial of order 3:\n",
    "\n",
    "$$\n",
    "f(x)=4-3x-2x^2+3x^3\n",
    "$$\n",
    "for $x \\in [-1,1]$.\n",
    "\n",
    "In order to make practice with NN, explore reasonable different choices for:\n",
    "\n",
    "- the number of layers\n",
    "- the number of neurons in each layer\n",
    "- the activation function\n",
    "- the optimizer\n",
    "- the loss function\n",
    "  \n",
    "Make graphs comparing fits for different NNs.\n",
    "Check your NN models by seeing how well your fits predict newly generated test data (including on data outside the range you fit. How well do your NN do on points in the range of $x$ where you trained the model? How about points outside the original training data set? \n",
    "Summarize what you have learned about the relationship between model complexity (number of parameters), goodness of fit on training data, and the ability to predict well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the code for trying the model for num_layers_list = [1, 2, 3], num_neurons_list = [10, 20, 30], activation_list = ['relu', 'tanh', 'sigmoid'] and optimizer='adam'; The code will just the best set or parameters based on results, mse and print those parameters. The code will perform using adam optimizer and MES loss function. I will write the resolts for other optimizer like: 'sgd', 'rmsprop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To evaluate how well the neural network perform on points where it was trained and outside the original training data set, i will generate test for both. And print the results as for the prediction and finally print the MSE results. A lower 'MSE Training Range' better the model perform on data in range (in this case [-1,1]). A lower 'MSE Outside Training Range better the model generalize to data point outside of the original training range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Function to generate data for the function\n",
    "def generate_data(num_samples, noise_level):\n",
    "    X = np.linspace(-1, 1, num_samples)\n",
    "    y = 4 - 3*X - 2*X**2 + 3*X**3 + np.random.normal(0, noise_level, num_samples)\n",
    "    return X, y\n",
    "\n",
    "#Function to build the model with fixed configurations\n",
    "def build_model(num_layers, num_neurons, activation):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=1, activation=activation))\n",
    "    for _ in range(num_layers - 1):\n",
    "        model.add(Dense(num_neurons, activation=activation))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "#Function to train the model\n",
    "def train_model(model, X_train, y_train, num_epochs):\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, verbose=0)\n",
    "    return history\n",
    "\n",
    "#Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "# Parameters\n",
    "num_epochs = 200\n",
    "num_train_samples = 500\n",
    "noise_level = 0.3\n",
    "\n",
    "# Generate noisy data\n",
    "X, y = generate_data(num_train_samples, noise_level)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model configurations to explore\n",
    "num_layers_list = [1, 2, 3]\n",
    "num_neurons_list = [10, 20, 30]\n",
    "activation_list = ['relu', 'tanh', 'sigmoid']\n",
    "optimizer = 'adam'\n",
    "\n",
    "# Train and evaluate models for different configurations\n",
    "results = {}\n",
    "for num_layers in num_layers_list:\n",
    "    for num_neurons in num_neurons_list:\n",
    "        for activation in activation_list:\n",
    "            model = build_model(num_layers, num_neurons, activation)\n",
    "            train_model(model, X_train, y_train, num_epochs)\n",
    "            mse = evaluate_model(model, X_test, y_test)\n",
    "            results[(num_layers, num_neurons, activation)] = mse\n",
    "\n",
    "# Find the best model based on MSE\n",
    "best_config = min(results, key=results.get)\n",
    "best_mse = results[best_config]\n",
    "best_model = build_model(*best_config)\n",
    "\n",
    "print(\"Best Configuration (Layers, Neurons, Activation):\", best_config)\n",
    "print(\"Best Mean Squared Error:\", best_mse)\n",
    "\n",
    "# Train the best model on the entire dataset for the final fit\n",
    "best_model.fit(X, y, epochs=num_epochs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results for the best model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='True Data')\n",
    "plt.scatter(X, best_model.predict(X), color='red', label='Predicted Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regression using Neural Network (Best Model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate test data points within and outside the training range\n",
    "X_test_within_range, y_test_within_range = generate_data(1000, noise_level)\n",
    "X_test_outside_range, y_test_outside_range = generate_data(500, noise_level)\n",
    "\n",
    "#Evaluate the best model on the test data within the training range\n",
    "mse_within_range = evaluate_model(best_model, X_test_within_range, y_test_within_range)\n",
    "\n",
    "#Evaluate the best model on the test data outside the training range\n",
    "mse_outside_range = evaluate_model(best_model, X_test_outside_range, y_test_outside_range)\n",
    "\n",
    "print(\"MSE on Test Data Within Training Range:\", mse_within_range)\n",
    "print(\"MSE on Test Data Outside Training Range:\", mse_outside_range)\n",
    "\n",
    "#Plot the results for the best model on the training data and test data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='True Data')\n",
    "plt.scatter(X, best_model.predict(X), color='red', label='Predicted Data (Training Range)')\n",
    "plt.scatter(X_test_outside_range, best_model.predict(X_test_outside_range), color='green', label='Predicted Data (Outside Training Range)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regression using Neural Network (Best Model)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 'sgd' optimizer:\n",
    "### Best Configuration (Layers, Neurons, Activation): (3, 10, 'tanh')\n",
    "### Best Mean Squared Error: 0.11295744976260949\n",
    "### MSE on Test Data Within Training Range: 0.09669805007726683\n",
    "### MSE on Test Data Outside Training Range: 0.09126866799679019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 'rmsprop' optimizer:\n",
    "### Best Configuration (Layers, Neurons, Activation): (3, 30, 'tanh')\n",
    "### Best Mean Squared Error: 0.09761959024600651\n",
    "### MSE on Test Data Within Training Range: 0.09793653127853039\n",
    "### MSE on Test Data Outside Training Range: 0.09567728597594435"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 'nadam' optimizer:\n",
    "### Best Configuration (Layers, Neurons, Activation): (3, 20, 'relu')\n",
    "### Best Mean Squared Error: 0.08407031621248665\n",
    "### MSE on Test Data Within Training Range: 0.08881880940391397\n",
    "### MSE on Test Data Outside Training Range: 0.09566193529325659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 11 the file it's named Esercizio1.2.5.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here the same code with fixed parameters: layer 3, 30 neurons , activation function 'tanh', optimizer 'rmsprop'. I add the PlotCurrentEstimate for this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Function to generate data for the function\n",
    "def generate_data(num_samples, noise_level):\n",
    "    X = np.linspace(-1, 1, num_samples)\n",
    "    y = 4 - 3*X - 2*X**2 + 3*X**3 + np.random.normal(0, noise_level, num_samples)\n",
    "    return X, y\n",
    "\n",
    "#Function to build the model with fixed configurations\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=1, activation='tanh'))\n",
    "    model.add(Dense(30, activation='tanh'))\n",
    "    model.add(Dense(30, activation='tanh'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop')\n",
    "    return model\n",
    "#Function to train the model\n",
    "def train_model(model, X_train, y_train, num_epochs, x_valid, y_valid):\n",
    "    #Custom callback to plot the current model estimate during training\n",
    "    class PlotCurrentEstimate(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, x_valid, y_valid):\n",
    "            self.x_valid = x_valid\n",
    "            self.y_valid = y_valid\n",
    "            self.iter = 0\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            temp = self.model.predict(self.x_valid, batch_size=None, verbose=False, steps=None)\n",
    "            self.y_curr = temp.flatten()\n",
    "\n",
    "            self.iter += 1\n",
    "            if self.iter % 10 == 0:\n",
    "                plt.clf()\n",
    "                plt.scatter(self.x_valid, self.y_curr, color=\"blue\", s=4, marker=\"o\", label=\"estimate\")\n",
    "                plt.scatter(self.x_valid, self.y_valid, color=\"red\", s=4, marker=\"x\", label=\"valid\")\n",
    "                plt.legend()\n",
    "                plt.pause(0.05)\n",
    "\n",
    "    #Custom callback for early stopping based on validation loss\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(x_valid, y_valid),\n",
    "                        callbacks=[PlotCurrentEstimate(x_valid, y_valid), early_stopping], verbose=0)\n",
    "    return history\n",
    "\n",
    "#Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "#Parameters\n",
    "num_epochs = 200\n",
    "num_train_samples = 500\n",
    "noise_level = 0.3\n",
    "\n",
    "#Generate noisy data\n",
    "X, y = generate_data(num_train_samples, noise_level)\n",
    "#Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#Build the model\n",
    "model = build_model()\n",
    "#Train the model\n",
    "train_model(model, X_train, y_train, num_epochs, X_test, y_test)\n",
    "\n",
    "#Evaluate the model on the test data\n",
    "mse_test = evaluate_model(model, X_test, y_test)\n",
    "\n",
    "#Print the MSE on the test data\n",
    "print(\"Mean Squared Error on Test Data:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the results for the best model on the training data and test data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='True Data')\n",
    "plt.scatter(X, model.predict(X), color='red', label='Predicted Data (Training Range)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Regression using Neural Network')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 11 the file it's named Esercizio1.3.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11.3\n",
    "  \n",
    "Try to extend the model to fit a simple trigonometric 2D function such as $f(x,y) = \\sin(x^2+y^2)$ in the range $x \\in [-3/2,3/2]$ and $y \\in [-3/2,3/2]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K\n",
    "from keras.utils import get_custom_objects\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Function to generate data for the trigonometric 2D function\n",
    "def generate_data(num_samples, noise_level):\n",
    "    x = np.linspace(-3/2, 3/2, num_samples)\n",
    "    y = np.linspace(-3/2, 3/2, num_samples)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    Z = np.sin(X**2 + Y**2) + np.random.normal(0, noise_level, (num_samples, num_samples))\n",
    "    return X, Y, Z\n",
    "\n",
    "#Function to build the model with fixed configurations\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=2, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(20, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='nadam')\n",
    "    return model\n",
    "\n",
    "#Function to train the model\n",
    "def train_model(model, X_train, y_train, num_epochs, x_valid, y_valid):\n",
    "    #Custom callback to plot the current estimate during training\n",
    "    class PlotCurrentEstimate(tf.keras.callbacks.Callback):\n",
    "        def __init__(self, x_valid, y_valid):\n",
    "            self.x_valid = x_valid\n",
    "            self.y_valid = y_valid\n",
    "            self.iter = 0\n",
    "\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            temp = self.model.predict(self.x_valid, batch_size=None, verbose=False, steps=None)\n",
    "            self.y_curr = temp.flatten()\n",
    "\n",
    "            self.iter += 1\n",
    "            if self.iter % 10 == 0:\n",
    "                plt.clf()\n",
    "                plt.scatter(self.x_valid[:, 0], self.y_curr, color=\"blue\", s=4, marker=\"o\", label=\"estimate\")\n",
    "                plt.scatter(self.x_valid[:, 0], self.y_valid, color=\"red\", s=4, marker=\"x\", label=\"valid\")\n",
    "                plt.legend()\n",
    "                plt.pause(0.05)\n",
    "\n",
    "    #Custom callback for early stopping based on validation loss\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(x_valid, y_valid),\n",
    "                        callbacks=[PlotCurrentEstimate(x_valid, y_valid), early_stopping], verbose=0)\n",
    "    return history\n",
    "\n",
    "#Function to evaluate the model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse\n",
    "\n",
    "#Parameters\n",
    "num_epochs = 200\n",
    "num_train_samples = 50\n",
    "noise_level = 0.1\n",
    "\n",
    "#Generate noisy data for the trigonometric 2D function\n",
    "X, Y, Z = generate_data(num_train_samples, noise_level)\n",
    "#Flatten the 2D arrays to use as input for the model\n",
    "X_train = np.column_stack((X.ravel(), Y.ravel()))\n",
    "y_train = Z.ravel()\n",
    "#Split the data into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "#Build the model with fixed configurations\n",
    "model = build_model()\n",
    "#Train the model\n",
    "train_model(model, X_train, y_train, num_epochs, X_valid, y_valid)\n",
    "\n",
    "#Plot the results\n",
    "X_plot, Y_plot = np.meshgrid(np.linspace(-3/2, 3/2, 100), np.linspace(-3/2, 3/2, 100))\n",
    "Z_true = np.sin(X_plot**2 + Y_plot**2)\n",
    "Z_pred_flat = model.predict(np.column_stack((X_plot.ravel(), Y_plot.ravel())))\n",
    "Z_pred = Z_pred_flat.reshape(X_plot.shape)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "#True Function and Predicted Function Surface Plot\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "true_plot = ax.plot_surface(X_plot, Y_plot, Z_true, cmap='viridis', alpha=0.7, label='True Function')\n",
    "pred_plot = ax.plot_surface(X_plot, Y_plot, Z_pred, cmap='plasma', alpha=0.7, label='Predicted Function')\n",
    "\n",
    "#Add a colorbar to indicate the mapping of the true and predicted functions\n",
    "fig.colorbar(true_plot, ax=ax, shrink=0.6)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('f(X, Y)')\n",
    "ax.set_title('True Function vs. Predicted Function')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this updated code, we modify the generate_data function to generate 2D data points in the range specified. The function now returns three arrays: X, Y, and Z, representing the input coordinates and the corresponding output values. The model architecture is modified to handle 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideas to meditate on these exercises and judge your results can be found <a href=https://xkcd.com/2048/>here</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary material: Keras model.fit available callbacks</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .fit method can also get <a href=https://keras.io/callbacks/>callback</a> functions which can be used to customize the fitting procedure with special actions.\n",
    "\n",
    "Keras provides some predefined callbacks to feed in, among them for example:\n",
    "- **TerminateOnNaN()**: that terminates training when a NaN loss is encountered\n",
    "- **ModelCheckpoint(filepath)**: that save the model after every epoch\n",
    "- **EarlyStopping()**: which stop training when a monitored quantity has stopped improving\n",
    "\n",
    "You can select one or more callback and pass them as a list to the callback argument of the fit method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to construct a callback object to represent how estimated parameters are converging during the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "class PlotCurrentEstimate(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, x_valid, y_valid):\n",
    "        \"\"\"Keras Callback which plot current model estimate against reference target\"\"\"\n",
    "        \n",
    "        # convert numpy arrays into lists for plotting purposes\n",
    "        self.x_valid = list(x_valid[:])\n",
    "        self.y_valid = list(y_valid[:])\n",
    "        self.iter=0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        temp = self.model.predict(self.x_valid, batch_size=None, verbose=False, steps=None)\n",
    "        self.y_curr = list(temp[:]) # convert numpy array into list\n",
    "        \n",
    "        self.iter+=1\n",
    "        if self.iter%10 == 0:\n",
    "            clear_output(wait=True)            \n",
    "            self.eplot = plt.subplot(1,1,1)\n",
    "            self.eplot.clear()     \n",
    "            self.eplot.scatter(self.x_valid, self.y_curr, color=\"blue\", s=4, marker=\"o\", label=\"estimate\")\n",
    "            self.eplot.scatter(self.x_valid, self.y_valid, color=\"red\", s=4, marker=\"x\", label=\"valid\")\n",
    "            self.eplot.legend()\n",
    "\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use also an EarlyStopping callback on the val_loss quantity. This will stop the training process as soon as the val_loss quantity does not improve anymore after an amount of epochs, preventing a long time of wated computation to take over without useful results.\n",
    "\n",
    "<code>keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)</code>\n",
    "\n",
    "Arguments:\n",
    "\n",
    "- <code>monitor</code>: quantity to be monitored. \n",
    "- <code>min_delta:</code> minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement. \n",
    "- <code>patience:</code> number of epochs with no improvement after which training will be stopped. \n",
    "- <code>verbose:</code> verbosity mode. \n",
    "- <code>mode:</code> one of {auto, min, max}. In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. \n",
    "- <code>baseline:</code> Baseline value for the monitored quantity to reach. Training will stop if the model doesn't show improvement over the baseline. \n",
    "- <code>restore_best_weights:</code> whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_estimate = PlotCurrentEstimate(x_valid, y_valid)\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                  min_delta=0, patience=100, mode='auto')\n",
    "\n",
    "model.fit(x_valid, y_valid, batch_size=32, epochs=150,\n",
    "          validation_data=(x_valid, y_valid),\n",
    "          callbacks=[ plot_estimate, earlystop]\n",
    "          )\n",
    "\n",
    "model.get_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
