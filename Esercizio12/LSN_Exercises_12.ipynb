{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:brown\"> Python Exercise 12 </span>  Igor Vasiljevic 11191A\n",
    "## <span style=\"color:orange\"> Keras - Deep & Convolutional Neural Network image recognition </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note that, i performed the code on local python edit. So i will add the code .py inside the folder. The problem is that the kernel died on jupyter lab (even your code), so to avoid the problem (after a lot of time) i decide to add the program in .py (i test the code) and copy the code inside here and hopes that will do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST with Keras\n",
    "\n",
    "The goal of exercise 12 is to use deep neural network models, implemented in the Keras python package, to recognize and distinguish between the ten handwritten digits (0-9).\n",
    "\n",
    "The MNIST dataset comprises $70000$ handwritten digits, each of which comes in a square image, divided into a $28\\times 28$ pixel grid. Every pixel can take on $256$ gradation of the gray color, interpolating between white and black, and hence each data point assumes any value in the set $\\{0,1,\\dots,255\\}$. Since there are $10$ categories in the problem, corresponding to the ten digits, this problem represents a generic **classification task**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Load and Process the Data\n",
    "\n",
    "Keras can conveniently download the MNIST data from the web. All we need to do is import the `mnist` module and use the `load_data()` class, and it will create the training and test data sets or us.\n",
    "\n",
    "The MNIST set has pre-defined test and training sets, in order to facilitate the comparison of the performance of different models on the data.\n",
    "\n",
    "Once we have loaded the data, we need to format it in the correct shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape data and convert labels to be used with categorical cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 2: Define the Neural Net and its Architecture\n",
    "\n",
    "We can now move on to construct our deep neural net. We shall use Keras's `Sequential()` class to instantiate a model, and will add different deep layers one by one using the `add()` method\n",
    "\n",
    "For the purposes of our example, it suffices to focus on `Dense` layers for simplicity. Every `Dense()` layer accepts as its first required argument an integer which specifies the number of neurons. The type of activation function for the layer is defined using the `activation` optional argument, the input of which is the name of the activation function in `string` format. Examples include `relu`, `tanh`, `elu`, `sigmoid`, `softmax`. \n",
    "\n",
    "In order for our DNN to work properly, we have to make sure that the numbers of input and output neurons for each layer match. Therefore, we specify the shape of the input in the first layer of the model explicitly using the optional argument `input_shape=(N_features,)`. The sequential construction of the model then allows Keras to infer the correct input/output dimensions of all hidden layers automatically. Hence, we only need to specify the size of the softmax output layer to match the number of categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 3: Choose the Optimizer and the Cost Function\n",
    "\n",
    "Next, we choose the loss function according to which to train the DNN. For classification problems, this is the cross entropy, and since the output data was cast in categorical form, we choose the `categorical_crossentropy` defined in Keras' `losses` module. Depending on the problem of interest one can pick any other suitable loss function. To optimize the weights of the net, we choose SGD. This algorithm is already available to use under Keras' `optimizers` module, but we could use any other built-in one as well. The parameters for the optimizer, such as `lr` (learning rate) or `momentum` are passed using the corresponding optional arguments of the `SGD()` function. All available arguments can be found in Keras' online documentation at [https://keras.io/](https://keras.io/). While the loss function and the optimizer are essential for the training procedure, to test the performance of the model one may want to look at a particular `metric` of performance. For instance, in categorical tasks one typically looks at their `accuracy`, which is defined as the percentage of correctly classified data points. To complete the definition of our model, we use the `compile()` method, with optional arguments for the `optimizer`, `loss`, and the validation `metric` as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 4: Train the model\n",
    "\n",
    "We train our DNN in minibatches. \n",
    "\n",
    "Shuffling the training data during training improves stability of the model. Thus, we train over a number of training epochs. Each epoch corresponds to using **all the train data** divided in minibatches.\n",
    "\n",
    "Training the DNN is a one-liner using the `fit()` method of the `Sequential` class. The first two required arguments are the training input and output data. As optional arguments, we specify the mini-`batch_size`, the number of training `epochs`, and the test or `validation_data`. To monitor the training procedure for every epoch, we set `verbose=True`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Step 5: Evaluate the Model Performance on the *Unseen* Test Data\n",
    "\n",
    "Next, we evaluate the model and read of the loss on the test data, and its accuracy using the `evaluate()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.1\n",
    "\n",
    "<span style=\"color:red\">Change at will and train your DNN by increasing the number of epochs to an adeuqate value</span>. Try to use at least two other optimizers, different from SGD: <span style=\"color:red\">watch to accuracy and loss for training and validation data and comment on the performances</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 12 the file it's named Esercizio2.1.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True' #This is needed in my Anaconda+MacOsX installation; leave it commented.\n",
    "\n",
    "def create_DNN():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400, input_shape=(img_rows * img_cols,), activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def compile_model(optimizer):\n",
    "    model = create_DNN()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Reshape and normalize the input data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows * img_cols)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows * img_cols)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "#Convert the class labels to categorical format\n",
    "Y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "Y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 32\n",
    "epochs = 20  # Increase the number of epochs\n",
    "\n",
    "#Create and compile the models with different optimizers\n",
    "model_DNN_sgd = compile_model(SGD())\n",
    "model_DNN_adam = compile_model(Adam())\n",
    "model_DNN_rmsprop = compile_model(RMSprop())\n",
    "\n",
    "#Train the DNNs and store training info in history\n",
    "history_sgd = model_DNN_sgd.fit(X_train, Y_train,\n",
    "                                batch_size=batch_size,\n",
    "                                epochs=epochs,\n",
    "                                verbose=1,\n",
    "                                validation_data=(X_test, Y_test))\n",
    "\n",
    "history_adam = model_DNN_adam.fit(X_train, Y_train,\n",
    "                                  batch_size=batch_size,\n",
    "                                  epochs=epochs,\n",
    "                                  verbose=1,\n",
    "                                  validation_data=(X_test, Y_test))\n",
    "\n",
    "history_rmsprop = model_DNN_rmsprop.fit(X_train, Y_train,\n",
    "                                        batch_size=batch_size,\n",
    "                                        epochs=epochs,\n",
    "                                        verbose=1,\n",
    "                                        validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation accuracy for each optimizer\n",
    "plt.plot(history_sgd.history['accuracy'])\n",
    "plt.plot(history_sgd.history['val_accuracy'])\n",
    "plt.plot(history_adam.history['accuracy'])\n",
    "plt.plot(history_adam.history['val_accuracy'])\n",
    "plt.plot(history_rmsprop.history['accuracy'])\n",
    "plt.plot(history_rmsprop.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['SGD_train', 'SGD_val', 'Adam_train', 'Adam_val', 'RMSprop_train', 'RMSprop_val'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation loss for each optimizer\n",
    "plt.plot(history_sgd.history['loss'])\n",
    "plt.plot(history_sgd.history['val_loss'])\n",
    "plt.plot(history_adam.history['loss'])\n",
    "plt.plot(history_adam.history['val_loss'])\n",
    "plt.plot(history_rmsprop.history['loss'])\n",
    "plt.plot(history_rmsprop.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['SGD_train', 'SGD_val', 'Adam_train', 'Adam_val', 'RMSprop_train', 'RMSprop_val'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Optimizer: The training accuracy guadually increases with Epoch, the model is learning from the traning data; but it seems that the convergence is slowly and the final accuracy is lower compared to other optimizers. The validation accuracy follow a similar trend to training accuracy, but increases slowly; the final value is lower compared to other. The traning loss decreases constantly during the traning; this indicate that the model is effictively learing to minimaze the error. The validation loss decreases slowly; this indicate that the model may not be generalizing the unseen data.\n",
    "## Adam Optimizer: The training accuracy rapidly increases with Epoch and reaching the high value. The validation accuracy follow the training accuracy; good generalization to unseen data. The training loss decreases rapidly, so the model quickly learns from the traning data and minimizes the error. The validation loss seems constant during each epoch.\n",
    "## RMSprop Optimizer: The training accuracy increases constatly during the proses, but not as good as Adam. Even the final value is lower then Adam's. The validation accuracy follows closly the traning accuracy, almost reaching the final value. The traning loss initialy decreases but then seems to slowly increses. The validation loss seems to increase with each Epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall the best performance are to Adam and RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 11 the file it's named Esercizio2.2.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12.2\n",
    "\n",
    "Change the architecture of your DNN using convolutional layers. Use `Conv2D`, `MaxPooling2D`, `Dropout`, but also do not forget `Flatten`, a standard `Dense` layer and `soft-max` in the end. I have merged step 2 and 3 in the following definition of `create_CNN()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True' #This is needed in my Anaconda+MacOsX installation; leave it commented.\n",
    "\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(img_rows, img_cols, 1)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def compile_model(optimizer):\n",
    "    model = create_CNN()\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Reshape and normalize the input data\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "#Convert the class labels to categorical format\n",
    "Y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "Y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 32\n",
    "epochs = 20  # Increase the number of epochs\n",
    "\n",
    "#Create and compile the models with different optimizers\n",
    "model_CNN_sgd = compile_model(SGD())\n",
    "model_CNN_adam = compile_model(Adam())\n",
    "model_CNN_rmsprop = compile_model(RMSprop())\n",
    "\n",
    "#Train the CNNs and store training info in history\n",
    "history_cnn_sgd = model_CNN_sgd.fit(X_train, Y_train,\n",
    "                                    batch_size=batch_size,\n",
    "                                    epochs=epochs,\n",
    "                                    verbose=1,\n",
    "                                    validation_data=(X_test, Y_test))\n",
    "\n",
    "history_cnn_adam = model_CNN_adam.fit(X_train, Y_train,\n",
    "                                      batch_size=batch_size,\n",
    "                                      epochs=epochs,\n",
    "                                      verbose=1,\n",
    "                                      validation_data=(X_test, Y_test))\n",
    "\n",
    "history_cnn_rmsprop = model_CNN_rmsprop.fit(X_train, Y_train,\n",
    "                                            batch_size=batch_size,\n",
    "                                            epochs=epochs,\n",
    "                                            verbose=1,\n",
    "                                            validation_data=(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation accuracy for each optimizer\n",
    "plt.plot(history_cnn_sgd.history['accuracy'])\n",
    "plt.plot(history_cnn_sgd.history['val_accuracy'])\n",
    "plt.plot(history_cnn_adam.history['accuracy'])\n",
    "plt.plot(history_cnn_adam.history['val_accuracy'])\n",
    "plt.plot(history_cnn_rmsprop.history['accuracy'])\n",
    "plt.plot(history_cnn_rmsprop.history['val_accuracy'])\n",
    "plt.title('CNN Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['SGD_train', 'SGD_val', 'Adam_train', 'Adam_val', 'RMSprop_train', 'RMSprop_val'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the training and validation loss for each optimizer\n",
    "plt.plot(history_cnn_sgd.history['loss'])\n",
    "plt.plot(history_cnn_sgd.history['val_loss'])\n",
    "plt.plot(history_cnn_adam.history['loss'])\n",
    "plt.plot(history_cnn_adam.history['val_loss'])\n",
    "plt.plot(history_cnn_rmsprop.history['loss'])\n",
    "plt.plot(history_cnn_rmsprop.history['val_loss'])\n",
    "plt.title('CNN Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['SGD_train', 'SGD_val', 'Adam_train', 'Adam_val', 'RMSprop_train', 'RMSprop_val'], loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The exercise is in the folder Esercizio 11 the file it's named Esercizio2.3.py, here the copy and paste (hoping works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 12.3\n",
    "\n",
    "Use the `gimp` application to create 10 pictures of your \"handwritten\" digits, import them in your jupyter-notebook and try to see if your CNN is able to recognize your handwritten digits.\n",
    "\n",
    "For example, you can use the following code to import a picture of an handwritten digit\n",
    "(Note: you should install Python Image Library (PIL/Pillow):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The create_CNN() function is defined to create the CNN model architecture. It constructs a sequential model and adds layers such as convolutional, max pooling, dropout, flatten, dense, and softmax layers. The model is then compiled with the specified loss function, optimizer, and metrics.\n",
    "# The MNIST dataset is loaded using mnist.load_data(), and the input data is reshaped and normalized.\n",
    "# The model is trained using the fit() function, providing the training data, batch size, number of epochs, and validation data.\n",
    "# The code then load the images of handwritten (writen with GIMP) digits using PIL. Each image is converted to grayscale, resized to 28x28 pixels, converted to a NumPy array, and normalized to have pixel values between 0 and 1.\n",
    "# The loaded images are reshaped\n",
    "# The trained CNN model is used to predict the labels for the handwritten digit images by calling the predict() function\n",
    "# The predicted labels are extracted by finding the index of the highest probability using np.argmax().\n",
    "# The predicted labels for each image are printed \n",
    "# The variable test_index is set to specify the index of the test image for which activations will be visualized.\n",
    "# The activation_model is defined a new model using the same input and output layers as the model_CNN.\n",
    "# The activations for the specified test image are obtained by calling predict() on the activation_model, passing the reshaped test image.\n",
    "# The display_activation() function for display the activations of a specific layer. It takes the activations, the desired number of columns and rows for the subplot grid, and the index of the layer as inputs.\n",
    "# The original test image is displayed using plt.imshow().\n",
    "# The display_activation() function is called with the activations, the desired grid dimensions (4x2), and the index of the first layer (0). This displays the activations of the first layer in a grid of subplots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Adam optimizer; epochs are fixed to 30, I tried a lot of values and different type of images of numbers. For some reason the CNN cant predict the 9 with less then 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True' #This is needed in my Anaconda+MacOsX installation; leave it commented.\n",
    "\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "#Function to display the activations\n",
    "def display_activation(activations, col_size, row_size, layer_index):\n",
    "    activation = activations[layer_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*3,col_size*3))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1\n",
    "\n",
    "#Load MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Reshape the input data\n",
    "if keras.backend.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "    input_shape = (1, 28, 28)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "    input_shape = (28, 28, 1)\n",
    "\n",
    "#Convert the data type to float32 and normalize the pixel values\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "#Convert the class labels to categorical format\n",
    "Y_train = to_categorical(y_train, num_classes=10)\n",
    "Y_test = to_categorical(y_test, num_classes=10)\n",
    "\n",
    "#Training parameters\n",
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "#Create the deep conv net\n",
    "model_CNN = create_CNN()\n",
    "\n",
    "#Train CNN\n",
    "model_CNN.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, Y_test))\n",
    "\n",
    "#Load the images\n",
    "image_paths = ['digit0.png', 'digit1.png', 'digit2.png', 'digit3.png', 'digit4.png',\n",
    "               'digit5.png', 'digit6.png', 'digit7.png', 'digit8.png', 'digit9.png']\n",
    "\n",
    "images = []\n",
    "for path in image_paths:\n",
    "    image = Image.open(path).convert('L')  #Convert to grayscale\n",
    "    image = image.resize((28, 28))  #Resize to 28x28\n",
    "    image = np.array(image)  #Convert to numpy array\n",
    "    image = image / 255.0  #Normalize to have pixel values between 0 and 1\n",
    "    images.append(image)\n",
    "\n",
    "#Convert the images to a numpy array\n",
    "images = np.array(images)\n",
    "#Reshape the images to add a channel dimension\n",
    "images = np.expand_dims(images, axis=-1)\n",
    "#Use the trained CNN model to predict the handwritten digits\n",
    "predictions = model_CNN.predict(images)\n",
    "#Get the predicted labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "#Print the predicted labels\n",
    "for i, label in enumerate(predicted_labels):\n",
    "    print(f\"Image {i}: Predicted Label = {label}\")\n",
    "\n",
    "#Index of the test image to visualize activations\n",
    "test_index = 0\n",
    "#Create an activation model\n",
    "layer_outputs = [layer.output for layer in model_CNN.layers]\n",
    "activation_model = Model(inputs=model_CNN.input, outputs=layer_outputs)\n",
    "#Get the activations for the test image\n",
    "activations = activation_model.predict(X_test[test_index].reshape(1,28,28,1))\n",
    "\n",
    "plt.imshow(X_test[test_index][:,:,0], cmap='gray')\n",
    "plt.show()\n",
    "display_activation(activations, 4, 2, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary information 1: Use of `gimp` </span>\n",
    "\n",
    "- from the Unix shell type: `gimp` and hit `Return`\n",
    "- File -> new (chose: 28x28 pixels)\n",
    "- rascale the image to 800%\n",
    "- Use the **brush** with dimension 2px to draw your digit\n",
    "- Color -> invert (to have black background)\n",
    "- Export the image as a `.png` file\n",
    "\n",
    "That's all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary information 2: Display trained filters in your CNN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print your NN layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_CNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m layer_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_CNN\u001b[49m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(layer_index, layer\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m      4\u001b[0m     layer_index\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_CNN' is not defined"
     ]
    }
   ],
   "source": [
    "layer_index=0\n",
    "for layer in model_CNN.layers:\n",
    "    print(layer_index, layer.name)\n",
    "    layer_index+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display your filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer_index should be the index of a convolutional layer\n",
    "layer_index=0\n",
    "# retrieve weights from the convolutional hidden layer\n",
    "filters, biases = model_CNN.layers[layer_index].get_weights()\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "print(filters.shape)\n",
    "\n",
    "# plot filters\n",
    "n_filters, ix = filters.shape[3], 1\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # specify subplot and turn of axis\n",
    "    ax = plt.subplot(1,n_filters, ix)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    # plot filter channel in grayscale\n",
    "    plt.imshow(f[:, :, 0], cmap='gray')\n",
    "    ix += 1\n",
    "# show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:green\">Supplementary information 3: Monitor layer outputs in your CNN</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = 0\n",
    "\n",
    "from keras.models import Model\n",
    "layer_outputs = [layer.output for layer in model_CNN.layers]\n",
    "activation_model = Model(inputs=model_CNN.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict(X_test[test_index].reshape(1,28,28,1))\n",
    " \n",
    "def display_activation(activations, col_size, row_size, layer_index): \n",
    "    activation = activations[layer_index]\n",
    "    activation_index=0\n",
    "    fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*3,col_size*3))\n",
    "    for row in range(0,row_size):\n",
    "        for col in range(0,col_size):\n",
    "            ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
    "            activation_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.imshow(X_test[test_index][:,:,0], cmap='gray')\n",
    "# def display_activation(activations, col_size, row_size, layer number)\n",
    "display_activation(activations, 4, 2, 0)\n",
    "# col_size x row_size must be <= Number of filters for the convolutional layer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
